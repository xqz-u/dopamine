import dopamine.discrete_domains.run_experiment
import dopamine.discrete_domains.gym_lib

import thesis.jax.agents.dqv_family.dqv_max_agent
import thesis.jax.networks
import thesis.experiment_data
import thesis.utils

# experiment config, use defaults for the rest
ExperimentData.seed = 42
ExperimentData.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
ExperimentData.min_replay_history = 500
ExperimentData.target_update_period = 100
ExperimentData.batch_size = 128
ExperimentData.checkpoint_dir = "/home/xqz-u/uni/fourthYear/bsc-thesis/dopamine/thesis/data_collection/JaxDQNAgent_ClassicControlDQNNetwork_CartPole-v0_1639221768/checkpoints"
ExperimentData.checkpoint_iterations = [496, 497, 498, 499]

# value and q function approximators
ClassicControlDNNetwork.min_vals = %jax_networks.CARTPOLE_MIN_VALS
ClassicControlDNNetwork.max_vals = %jax_networks.CARTPOLE_MAX_VALS

# agent
JaxDQVMaxAgent.state_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
JaxDQVMaxAgent.exp_data = @ExperimentData()
JaxDQVMaxAgent.V_network = @ClassicControlDNNetwork
JaxDQVMaxAgent.Q_network = @ClassicControlDNNetwork

# offline train runner
# these parameters are also used on the TrainRunner, but are configured only
# for its parent class Runner
Runner.num_iterations = 500
Runner.training_steps = 1000
# Runner.max_steps_per_episode = 200  # Default max episode length.
OfflineTrainRunner.create_environment_fn = @gym_lib.create_gym_environment
create_gym_environment.environment_name = 'CartPole'
create_gym_environment.version = 'v0'
create_agent.agent_name = "jax_dqv_max"
create_agent.debug_mode = True
create_runner.schedule = 'continuous_train'
create_runner.offline = True

make_unique_data_dir.experiment_spec = [@JaxDQVMaxAgent, @ClassicControlDQNNetwork, "CartPole-v0_offline"]
create_runner.base_dir = @make_unique_data_dir()
