import thesis.experiment_data
import dopamine.discrete_domains.run_experiment
import dopamine.discrete_domains.gym_lib
import thesis.jax.networks
import dopamine.jax.networks


# experiment config, use defaults for the rest
ExperimentData.seed = 42
ExperimentData.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
ExperimentData.min_replay_history = 500
ExperimentData.target_update_period = 100


# value and q function approximators
# NOTE not scoping 2 networks because I am using same parameters for both
# networks
ClassicControlDNNetwork.min_vals = %jax_networks.CARTPOLE_MIN_VALS
ClassicControlDNNetwork.max_vals = %jax_networks.CARTPOLE_MAX_VALS
ClassicControlDNNetwork.hidden_features = (512, 512)


# agent
JaxDQVAgent.state_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
JaxDQVAgent.exp_data = @ExperimentData()
JaxDQVAgent.V_network = @ClassicControlDNNetwork
JaxDQVAgent.Q_network = @ClassicControlDNNetwork


# train runner
# these parameters are also used on the TrainRunner, but are configured only
# for its parent class Runner
Runner.num_iterations = 20
Runner.training_steps = 1000
Runner.max_steps_per_episode = 200  # Default max episode length.
TrainRunner.create_environment_fn = @gym_lib.create_gym_environment

create_gym_environment.environment_name = 'CartPole'
create_gym_environment.version = 'v0'

create_agent.agent_name = "jax_dqv"
create_agent.debug_mode = True

create_runner.schedule = 'continuous_train'
