import dopamine.discrete_domains.run_experiment
import dopamine.discrete_domains.gym_lib

import thesis.jax.agents.dqv_family.dqv_agent
import thesis.jax.networks
import thesis.experiment_data
import thesis.utils

# experiment config, use defaults for the rest
ExperimentData.seed = 42
ExperimentData.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
ExperimentData.min_replay_history = 500
ExperimentData.target_update_period = 100

# value and q function approximators
ClassicControlDNNetwork.min_vals = %jax_networks.CARTPOLE_MIN_VALS
ClassicControlDNNetwork.max_vals = %jax_networks.CARTPOLE_MAX_VALS

# agent
JaxDQVAgent.state_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
JaxDQVAgent.exp_data = @ExperimentData()
JaxDQVAgent.V_network = @ClassicControlDNNetwork
JaxDQVAgent.Q_network = @ClassicControlDNNetwork

# memory
OutOfGraphReplayBuffer.observation_dtype = %jax_networks.CARTPOLE_OBSERVATION_DTYPE
OutOfGraphReplayBuffer.replay_capacity = 50000
OutOfGraphReplayBuffer.batch_size = 128

# train runner
# these parameters are also used on the TrainRunner, but are configured only
# for its parent class Runner
Runner.num_iterations = 500
Runner.training_steps = 1000
Runner.max_steps_per_episode = 200  # Default max episode length.
TrainRunner.create_environment_fn = @gym_lib.create_gym_environment
create_gym_environment.environment_name = 'CartPole'
create_gym_environment.version = 'v0'
create_agent.agent_name = "jax_dqv"
create_agent.debug_mode = True
create_runner.schedule = 'continuous_train'

make_unique_data_dir.experiment_spec = [@JaxDQVAgent, @ClassicControlDQNNetwork, "CartPole-v0"]
create_runner.base_dir = @make_unique_data_dir()
