#+options: toc:nil

#+author: Marco

bibliographystyle:apacite
bibliography:bibliography.bib

Question to Mattia, or look at Sutton's book: understand why online learning, so
with immediate updates, has highly correlated inputs and therefor does not
satisfy IID and is not suitable for stochastic learning methods e.g. NN

Intuition: if a history of transitions which contains each state-action-reward
etc tuple with equal probability is not provided, then the function approximator
will find patterns where there are none, overfitting the training data. It will
likely end up being able to approximate some game trajectories, since this are
highly correlated - what followed after what, and it is presented sequentially
in an online learning setting - and not being able to generalize them.


* A Deeper Look at Experience Replay
  cite:zhang2017deeper
** Introduction
   + Criticality: model complexity makes experience replay hard to explain
   + Exp. replay provides uncorrelated data to train a NN + improves data
     efficiency: why?
   + Default community value for memory buffer size: $10^6$, nobody bothers to
     tune this hyper-parameter but it is crucial for the success of an algorithm
   + Paper contribution:
     - systematic evaluation of exp. replay in tabular, linear and non linear
       function approximation
     - show that too small/too big memory buffer size produces bad results ->
       important and task specific parameter
     - propose method to tackle negative influence of too large memory buffer:
       always add last transition to batch of sampled ones (CER, Combined
       Experience Replay), O(1)
     - investigate all of this when using Q-Learning (DQN)
** Related work
   + CER is (inaccurately) a special case of PER, Prioritized Experience Replay,
     although they have different aims: the first wants to remedy for a memory
     buffer that is too large, the second to `replay the transitions in the
     buffer more efficiently'
   + Experience replay can be viewed as a planning method, because it is
     comparable to Dyna (Sutton) with a look-up table -> *TODO* look up Dyna!
   + Successful trials to eliminate exp. replay: AAAC (Asynchronous Advantage
     Actor-Critic) method, using parallelized workers with random seeds to
     sample transitions (?), so data is still uncorrelated
** Algorithms
   + Experience replay comes from cite:lin1992self.
   + Key idea: train an agent with transitions sampled from a set of previously
     experienced transitions.
     - *Transition*: quadruple $(s, a, r, s')$, state, action, reward, next
       state.
     - At each time step, the current transition is added to the replay buffer,
       and some transitions are sampled from it to train on; most used sampling
       strategy is /uniform sampling/, there is also /prioritized sampling/ but
       it has O(logN) time complexity; this paper only uses uniform sampling.
   + The paper compares 3 algorithms:
     1. =Online-Q=, Q-Learning with online transitions. Primitive Q-Learning,
        the transition at each time step updates the value function immediately
     2. =Buffer-Q=, Q-Learning with transitions for training only from the
        memory buffer. DQN-like learning, the current transition is not used
        immediately to update the value function; instead, it is added to the
        replay buffer, and only transitions sampled from it are used for
        learning (at each time step?)
     3. =Combined-Q=, Q-Learning with with CER, uses both the current transition
        and the ones sampled from the buffer to update the value function at
        each time step
** Testbeds
   + 3 environments:
     - grid world: agent spawns at same location on each episode, the goal
       location is always fixed, reward -1 at every time step, 4 cardinal
       directions as action set
     - Lunar Lander: state space $\mathbb{R}^8$, each dimension unbounded. Four
       discrete actions. Needs exploration: negative rewards received during
       landing, so the agent can get trapped in local maxima to avoid receiving
       negative rewards by simply doing nothing
     - Pong (Atari game): not using raw pixels as state, since the aim of the
       paper is to study exp. replay, not its interaction with a deep
       convolutional NN. Therefore they use more accurate representation: game
       RAM (?), so a state is a vector in $\{0, ..., 255\}^{128}$, and its entries
       are normalized to $[0, 1]$ dividing by 255. There are 6 discrete actions
   + All environments have a timeout
     - this makes the state-space distribution non-stationary; to account for
       this, they introduce very high timeouts, so that they are hopefully
       rarely reached
     - they also use PEB, Partial Episode Bootstrap (Pardo et. al 2017, get
       reference if needed): continue bootstrapping from the next state, when an
       episode ends by timeout, which Pardo shows reduces the negative influence
       of timeouts
   + Fixed batch-size of 10 transitions is used to sample from the replay buffer
     at each time step (9 for CER, the last one is the current transition)
   + Behavior policy: \epsilon-greedy at 0.1
   + Plots: online training progression (x = episode, y = episode's return),
     averaged over 30 runs
** Evaluation
*** Tabular Function Representation
    + Only the grid world is compatible with look-up tables (probably state
      space too big in other environments)
    + For =Buffer-Q=, small buffer size is better ($10^2$), if increased it
      ruins performance until re-stabilizing at $10^6$. This does not happen
      with =CER=, where performance stays similar across all buffer sizes, with
      $10^6$ being the fastest to find the solution. This suggests that CER
      makes algorithms that use exp. replay more resistant to buffer sizes
    + They derive some probability formula to show that with a large buffer
      size, a rare transition is likely to make influence later, so if this
      transition was important it will influence data collection in the future
      (see paper page 5).
      In contrast, with CER `all transitions influence the
      agent immediately' (this is because the current transition is always used,
      so if this was important it will not miss its spot until some later time)
*** Linear Function Approximation
    + Method: tile coding (? check it up)
    + Only Lunar Landing environment is compatible
    + Similar results as with tabular representation method
*** Non-linear Function Approximation
    + Function approximator: single hidden-layer NN, /Relu/ thresholding for the
      hidden units, linear activations for the output units to produce. 50
      hidden units in the grid world, 100 elsewhere, plus they empirically tune
      learning rates
      state-action value (Q value)
    + Almost the same as DQN, so they also employ a `target network to gain
      stable update targets following Mnih et. al 2015' (???)
    + RMSProp optimizer ???
    + Results:
      - Grid world, =Online-Q= (tabular Q-Learning) and =Buffer-Q= with buffer size 100 do
        not learn anything, expected since recent transitions are over-fit
        (there is literally no space for older transitions in the buffer)
      - Grid world, =Buffer-Q= size $10^4$ learns fast, hypothesis: trade off between data
        quality and data correlation. Small replay buffer means data is highly
        temporally correlated, and training a NN requires IID data (independent,
        identically distributed). Maybe this means that learning a policy is
        highly subject to the current data at hand, and that the span of the
        policy cannot reach information acquired in the past, i.e. it is blind
        about them? In fact, the authors state that large replay buffers provide
        uncorrelated, yet also outdated, data: indeed the =Buffer-Q= agent with
        largest memory buffers cannot find the optimal solution.
      - Lunar Landing world, =Online= and =Buffer= Q agents with buffer size 100
        perform good! Suggests that this environment is harder to over-fit for
        the NN. For this task, a medium buffer size seems optimal, while a huge
        one hurts performance in the =Buffer-Q= agent. Good news, CER helps
        diminish the negative effects in this case!
      - For the Pong task, CER does not provide optimization, but this is expected
        since the task is too hard to approximate with a single-layer NN
** Conclusion
   + `It is important to note that CER is only a workaround, the idea of
     experience replay itself is heavily flawed'
* Prioritized Experience Replay
  cite:schaul2015prioritized
** Introduction
   + Model free RL
   + Main issues with online RL algos:
     1. at one time step, they update their state-action value estimate and then
        discard the data immediately, resulting in `strongly correlated updates
        that break the i.i.d. assumption of many popular stochastic
        gradient-based algorithms'
     2. rare experiences are impossible to be valued, since their discarded
        immediately
   + Experience replay solution: ` _with experience stored in a replay memory, it_
     _becomes possible to break the temporal correlations by mixing more and less_
     _recent experience for the updates, and rare experience will be used for
     more than just a single update._'
   + Generally ER reduces the amount of experience needed to train an agent, and
     replaces them with computational demands and memory (e.g. in the original
     DQN each transition was re-played 8 times on average)
   + `Experience replay liberates online learning agents from processing
     transitions in the exact order they are experienced. Prioritized replay
     further liberates agents from considering transitions with the same
     frequency that they are experienced.' Same frequency since the same
     transition will be stored multiple times in the memory buffer, so randomly
     selecting it is proportional to the amount times it was experienced - its
     count in the buffer
   + Practically: try to more frequently re-play transitions with /high/
     /expected learning progress/, and to measure the latter the magnitude of a
     transition's TD error is employed -> *TODO* lookup TD-error once again, if
     I understand correctly, transitions were the expected reward was not in
     line with the actual one are then chosen more often!
   + To account for the loss of diversity in training data when using such
     prioritized transitions, they introduce:
     - /stochastic prioritizing/
     - /bias/, corrected for with /importance sampling/
** Background
   + Neuroscience studies suggest that some form of experience replay is
     performed by the /hippocampus/ of rodents, either in a awake resting or
     when sleeping (experiences associated with rewards and those with
     high-magnitude TD errors are re-played more often, it seems)
   + Some other stuff about regarding how TD error is used on other
     domains/problems to determine priorities to update some values, and
     regarding sampling techniques
** Prioritized Replay
   + Focus of this paper is one in a twofold approach towards exp. replay. Your
     design choices are related to:
     1. which experiences to store in the memory buffer
     2. how to sample the most relevant experiences and train on them
     This paper investigates only the second point
*** A Motivating Example
    + Look up ``Blind Cliffwalk" example
    + They propose a simple environment where it is impossible to generalize a
      strategy (they change the right and wrong action for a state between
      episodes), this to show the difference in learning times between an agent
      which samples experiences uniformly and one that uses an oracle - a
      function which selects an experience that maximally reduces the global
      loss in the current state -. Such an agent learns to solve the problem
      exponentially faster than the uniform sampling one; ofc such an agent is
      not feasible in the majority of environments, but it is a proof of concept
*** Prioritizing with TD-Error
    + The main idea is to find a selection criterion that enables to find a
      transition - or a batch of - that is best to replay. The idealized measure
      takes into account how much the agent can learn from a transition in its
      current state, so the /expected learning progress/; since this is unknown,
      a good proxy is the TD-error \delta, which indicates how unexpected a
      transition is (it actually measures how far the value estimated so far of
      a state, or of a state-action pair, is from the current one). Such a
      prioritization strategy is applied greedily, and it works really well and
      converges fast in the Blind Cliffwalk example
*** Stochastic Prioritization
    + Prioritized TD-error cons:
      1. To avoid recalculating \delta for all the replay buffer, which is
         expensive, only the \delta of the chosen transitions are updated; and
         since the selection is greedy over \delta, this means that fortuitously
         `expected' transitions, so those with low \delta, will be played less
         frequently, when the right action in that state could have indeed been
         chosen by accident
      2. Sensitive to noise spikes, e.g. stochastic rewards, which influence
         \delta (the reward is part of the TD update)
      3. Lack of diversity in training data due to greedy prioritization,
         leading to overfitting: unexpected transitions will be played most
         often, leaving no space to the other ones
    + Remedy: stochastic sampling, so softmax over the transitions' priorities
      to draw a sample. But how is a transition's priority determined? 2 ways:
      1. directly proportional to the transition's \delta, plus a small \epsilon
         to keep re-sampling transitions with \delta = 0
      2. indirect, rank based priority, where a transition's priority is
         defined as 1 over the transition's rank - and the rank is the
         transition's index in the replay buffer, sorted by \delta. They say
         that then the probability for a transition's to be sampled becomes a
         power law with exponent \alpha ????
    + Implementation is not super straightforward, see the appendix of the paper
      for details

*** Annealing the Bias
    + What happens is that having stochasticity in transitions selection (?)
      introduces bias, since it is not certain that the produced updates belong
      to the same distribution as their expected one (?); this is not the case
      with uniform sampling, because (...?) each transition has equal
      probability of being selected, so in the infinite limit all transitions
      will be used thus there is no bias toward any subset of them??
    + DID NOT GET THIS ????
* Revisiting Fundamentals of Experience Replay
  cite:fedus2020revisiting
  + Main goal of this study: investigate relationship between data generating
    mechanisms (here, experience replay) and learning algorithms
  + References both Sutton's CER and the PER work
** Disentangling Experience Replay
*** Independent Factors of Control
    Some factors related to the replay buffer:
    1. *Replay capacity*: the total number of transitions stored in the buffer
    2. *Oldest policy age*: age of the oldest transition in the replay buffer.
       This is defined as the number of gradient steps performed by the learner
       since the transition was generated
       - directly influenced by replay capacity
       - proxy for the degree of /off-policyness/ in the buffer (intuition is,
         the older a policy, the more likely it comes from a policy other than
         the current one)
    3. *Replay ratio*: number of gradient updates per environment transition, so
       relative frequency of between gradient updates and transitions (how often
       gradient updates are performed, e.g. 1 update after 4 transitions in the
       case of PER for a replay ratio of .25) -> *TODO* ask clarifications...
       - constant when increasing buffer size, since both the numerator and the
         denominator increase as well
       *TODO* ask about this!!
*** Experiments
    + They use the Dopamine Rainbow agent, a DQN based agent for their
      experiments, and then see if they can generalize the results to the
      original DQN agent
    + Vary 2 parameters: replay capacity and oldest policy (so for the second
      they vary the period of the gradient updates I guess?)
    + Increasing replay buffer size has always positive effects for the Rainbow
      agent, while the contrary is true for the DQN one; the authors will now
      investigate the dynamics of the two learners to understand /why/. They do
      so by incrementally adding elements of the Rainbow agent to the DQN one,
      and check the relative improvements due to different replay capacities
      - results show that it is the $n$-step return feature of the Rainbow agent
        that is responsible for the increase in performance resulting from
        increasing replay capacity! Also, they find that PER is not significant
        in this interaction

** Why is /n/-step the Enabling Factor?
   + Van Hasselt suggests that this way of calculating the estimated return is
     proficient because they make the magnitude of the bootstrap (the update to
     a Q value) smaller, since it now takes into account a longer time sequence
     than the normal return estimate
     - they hypothesize that the stabilizing effect of the $n$-step returns
       positively counterbalances the potential disruptive force of the
       off-polcyness introduced with a large buffer size. However, the
       experiments they perform to test this hypothesis fail to confirm it
     - another hypothesis related to the returns' variance... (?)
