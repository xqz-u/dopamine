+ make it so that, in my agents, =observation_dtype= is not needed,
  and is passed by the =create_agent_fn= after creating the
  environment in =dopamine/discrete_domains/run_experiment=
  DONE can be achieved in =thesis.constants=
+ is merging the replay buffers for the offline agents a good idea? if
  one wants to inspect the learning process, any reference to which
  loaded buffer the replayed actions originated from is lost
+ get rid of =ExperimentData=? DONE
+ check if DQN stores observations or states in replay memory, and if
  their shapes match DONE they store states
+ change folder structure pattern: keep my changes in a separate
  branch? DONE see =thesis.patcher=


* Logger
  + Agent has a list of logger classes. When these classes are called,
    they perform their logging. They can be configured beforehand to
    set up folder and their internals. *parameters passing* should be
    agnostic: best solution is Max's one. Analyze the call argspec,
    and pass the required args from wherever they are stored (agent?
    experiment data container? dict?).
    Goal -> loggers should be pluggable easily in an agent, given
    that they are properly configured. Ideally they should also be
    able to be stopped and reloaded
    DONE

* Agent
  + Base class for my agents DONE
  + Profile jax/training operations
  + DQV max and DQV DONE dqvmax
  + Train online on Atari games DONE
  + how about some parallelism? check when to use =jax.pmap=

* Runner
  + Dopamine has a dope runner with some useful functionality
    (checkpointing, logging and start/stop/reload ability). Its
    instantiation is quite strict, so maybe work on that... And in
    general try to keep configurations as python dictionaries, do not
    rely on gin DONE
    NOTE changed in new source! using gin config and relying on gin to
    save parameter values (it gives a config string, not as good as a
    dict but automatically collected)
    Moreover, no start and stop functionality exists now
  + Runner should be able to work with offline data as well! Unify the
    interfaces DONE
  + Ask Matthia for Peregrine access DONE

* Offline
  + First, choose whether to work with the Dopamine runner or not,
    since it already has a mechanism to dump the trajectories.
    If so  -> check if satisfied with current offline replay buffer
	      implementation DONE
    If not -> choose whether to keep working with the Dopamine replay
	      buffer, or to write a new one (consider also available
	      time...)
    DONE

* Data
  + Use a non-relational database e.g. =mongo= to store the data. This
    way, it is possible to collect data on one host and send them to a
    remote database through TCP
    NOTE this can involve both checkpointing and performance graphs,
    but needs care for how =aim= plays in this context
    NOTE implemented only for agents' performance tracking, not for
    memory or agents' checkpointing; TODO migrate everything! What is
    the point of having to care both of regular files and a db then?
    Portability gets more complicated, it is an incomplete design...

* General
  - [X] Parallelize redundancies code
  - [X] Ensemble DQVMax
  - [ ] Download the Atari DQN replay dataset on Peregrine
  - [ ] Produce distributional shift plot with DQVMax in atari games (?)
  - [ ] Move everything persistence-related to Mongo (?)
  - [X] Move the offline replay buffer classes to functions
  - [ ] Argparser to run in a unified way
  - [ ] all numpy that goes to jitted functions (mostly in
	replay_buffer) become jax.numpy

* Wrong design choices
  + the one related to the definition of model ts def: min and max
    vals to mlp depend on created env, but rn they cant easily be
    passed programmatically to mlp. this embeds env logic in a gin
    config file; need to understand how to create different
    configs programmitically for e.g. parallel runs to really
    understand how to use the gin config framework in a modular way -
    e.g. use predefined definitions of networks, optimezers etc. and
    only bind specific parameters
  + GIN ITSELF: don't have a dictionary of parameters anymore, which
    was especially useful in Aim to visualize runs!!! so it makes the
    latter less useful...

    # possible experiment specification form
# from thesis import constants, custom_pytrees, exploration, memory, networks

# x = lambda env, steps, iterations: {
#     "call_": runner.FixedBatchRunner,
#     "agent": {
#	"rng": {"call_": custom_pytrees.PRNGKeyWrap, "seed": 5},
#	"policy_evaluator": {
#	    "call_": exploration.Egreedy,
#	    "num_actions": env.environment.action_space.n,
#	},
#	"memory": {
#	    "call_": memory.load_offline_buffers,
#	    **constants.env_info(env),
#	    "replay_capacity": steps * iterations,
#	    "batch_size": 8,
#	},
#	"models": {
#	    "Q": {"call_": instantiators.create_model_TS_def, "model_def": ...},
#	    "V": {
#		"call_": instantiators.create_model_TS_def,
#		"model_def": (
#		    networks.MLP,
#		    {"features": 1, "hiddens": (4,), **constants.env_preproc_info(env)},
#		),
#	    },
#	},
#     },
# }


# plus, if I put exp_name here, can create a hash and pass it to
# reporters
# --runner
# run_hash (?)
# _checkpoint_dir
# iterations
# steps
# eval_period
# eval_steps
# redundancy
# policy_eval_callbacks_names (?)
# schedule
# f"{env.environment.spec.name}-{env.environment.spec.version}"


# --agent
# rng.seed
# clip_rewards
# gamma
# min_replay_history
# sync_weights_every
# training_period

# --memory
# observation_shape
# observation_dtype
# reward_dtype
# stack_size
# batch_size
# update_horizon
# replay_capacity

# --policy_evaluator
# all fields except model_call

# --models
# model_def (got it from the start)
# optimizer (with params)
# loss_fn
