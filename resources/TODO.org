+ make it so that, in my agents, =observation_dtype= is not needed,
  and is passed by the =create_agent_fn= after creating the
  environment in =dopamine/discrete_domains/run_experiment=
  DONE can be achieved in =thesis.constants=
+ is merging the replay buffers for the offline agents a good idea? if
  one wants to inspect the learning process, any reference to which
  loaded buffer the replayed actions originated from is lost
+ get rid of =ExperimentData=? DONE
+ check if DQN stores observations or states in replay memory, and if
  their shapes match DONE they store states
+ change folder structure pattern: keep my changes in a separate
  branch? DONE see =thesis.patcher=


* Logger
  + Agent has a list of logger classes. When these classes are called,
    they perform their logging. They can be configured beforehand to
    set up folder and their internals. *parameters passing* should be
    agnostic: best solution is Max's one. Analyze the call argspec,
    and pass the required args from wherever they are stored (agent?
    experiment data container? dict?).
    Goal -> loggers should be pluggable easily in an agent, given
    that they are properly configured. Ideally they should also be
    able to be stopped and reloaded
    DONE

* Agent
  + Base class for my agents DONE
  + Profile jax/training operations
  + DQV max and DQV DONE dqvmax
  + Train online on Atari games DONE
  + how about some parallelism? check when to use =jax.pmap=

* Runner
  + Dopamine has a dope runner with some useful functionality
    (checkpointing, logging and start/stop/reload ability). Its
    instantiation is quite strict, so maybe work on that... And in
    general try to keep configurations as python dictionaries, do not
    rely on gin DONE
  + Runner should be able to work with offline data as well! Unify the
    interfaces DONE
  + Ask Matthia for Peregrine access DONE

* Offline
  + First, choose whether to work with the Dopamine runner or not,
    since it already has a mechanism to dump the trajectories.
    If so  -> check if satisfied with current offline replay buffer
	      implementation DONE
    If not -> choose whether to keep working with the Dopamine replay
	      buffer, or to write a new one (consider also available
	      time...)
    DONE

* Data
  + Use a non-relational database e.g. =mongo= to store the data. This
    way, it is possible to collect data on one host and send them to a
    remote database through TCP
    NOTE this can involve both checkpointing and performance graphs,
    but needs care for how =aim= plays in this context
    NOTE implemented only for agents' performance tracking, not for
    memory or agents' checkpointing; TODO migrate everything! What is
    the point of having to care both of regular files and a db then?
    Portability gets more complicated, it is an incomplete design...

* General
  - [X] Parallelize redundancies code
  - [ ] Offline prioritized buffer: transfer all trajectories
	registered in some buffers to a prioritized one with equal
	priority, then train offline. Merge trajectories! Very data
	heavy, how to do this?
  - [ ] Ensemble DQVMax
  - [ ] Download the Atari DQN replay dataset on Peregrine
  - [ ] Produce distributional shift plot with DQVMax in atari games (?)
  - [ ] Load trained agent for evaluation (?)
  - [ ] Move everything persistence-related to Mongo (?)
  - [ ] Move the offline replay buffer classes to functions
  - [ ] Argparser to run in a unified way
  - [ ] Add paramater 'record_eval_q_values' to define a schedule to
	record Q-values, e.g. could be 'all' for every steps, or
	e.g. 'first' or 'last' for those particular steps of an episode;
	right now, we record every first step
