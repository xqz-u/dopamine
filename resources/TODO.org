+ make it so that, in my agents, =observation_dtype= is not needed,
  and is passed by the =create_agent_fn= after creating the
  environment in =dopamine/discrete_domains/run_experiment=
  DONE can be achieved in =thesis.constants=
+ is merging the replay buffers for the offline agents a good idea? if
  one wants to inspect the learning process, any reference to which
  loaded buffer the replayed actions originated from is lost
+ get rid of =ExperimentData=? DONE
+ check if DQN stores observations or states in replay memory, and if
  their shapes match DONE they store states
+ change folder structure pattern: keep my changes in a separate
  branch? DONE see =thesis.patcher=


NOTE my DQN agent learns slower than Dopamine's one, e.g. on CartPole!
need to check why again, although I am pretty sure my algorithm is
correct... but the number of total steps required is too high!!!
- check: try running Dopamine on cartpole with the parameters I use
  and see if performance is similar
  DONE running my agent with dopamine parameters works so much better!
  I think it is especially the network sizes which make the difference
- NOTE: dopamine dose not accumulate loss and then divide by #steps! I
  think the way I am plotting it is wrong, reason about it

* Logger
  + Agent has a list of logger classes. When these classes are called,
    they perform their logging. They can be configured beforehand to
    set up folder and their internals. *parameters passing* should be
    agnostic: best solution is Max's one. Analyze the call argspec,
    and pass the required args from wherever they are stored (agent?
    experiment data container? dict?).
    Goal -> loggers should be pluggable easily in an agent, given
    that they are properly configured. Ideally they should also be
    able to be stopped and reloaded
    DONE

* Agent
  + Base class for my agents DONE
  + Profile jax/training operations
  + DQV max and DQV DONE dqvmax
  + Train online on Atari games DONE
  + how about some parallelism? check when to use =jax.pmap=

* Runner
  + Dopamine has a dope runner with some useful functionality
    (checkpointing, logging and start/stop/reload ability). Its
    instantiation is quite strict, so maybe work on that... And in
    general try to keep configurations as python dictionaries, do not
    rely on gin DONE
    NOTE changed in new source! using gin config and relying on gin to
    save parameter values (it gives a config string, not as good as a
    dict but automatically collected)
    Moreover, no start and stop functionality exists now
  + Runner should be able to work with offline data as well! Unify the
    interfaces DONE
  + Ask Matthia for Peregrine access DONE

* Offline
  + First, choose whether to work with the Dopamine runner or not,
    since it already has a mechanism to dump the trajectories.
    If so  -> check if satisfied with current offline replay buffer
	      implementation DONE
    If not -> choose whether to keep working with the Dopamine replay
	      buffer, or to write a new one (consider also available
	      time...)
    DONE

* Data
  + Use a non-relational database e.g. =mongo= to store the data. This
    way, it is possible to collect data on one host and send them to a
    remote database through TCP
    NOTE this can involve both checkpointing and performance graphs,
    but needs care for how =aim= plays in this context
    NOTE implemented only for agents' performance tracking, not for
    memory or agents' checkpointing; TODO migrate everything! What is
    the point of having to care both of regular files and a db then?
    Portability gets more complicated, it is an incomplete design...

* TODOS
  - [X] Parallelize redundancies code
  - [X] Ensemble DQVMax, DQN, DQV
  - [ ] Produce distributional shift plot with DQVMax in atari games
  - [ ] Move everything persistence-related to Mongo (?)
  - [X] Move the offline replay buffer classes to functions
  - [ ] Argparser to run in a unified way
  - [ ] all numpy that goes to jitted functions (mostly in
	replay_buffer) becomes jax.numpy
  - [ ] Linode server that hosts Mongodb instance, which receives my
	data from Peregrine and forwards them locally through a ssh
	tunnel (?). Mostly for atari games, can run classic control
	ones on my pc
  - [ ] jax.pmap to better use cpu on peregrine for atari games! and
	not only, e.g. k=10 on classic control environments is very expensive...
  - [ ] shared body - different heads ensemble models!
  - [ ] record some videos to see how training progresses!!!
  - [ ] Gym's vectorized environment API - only useful for online
	training/evaluation though
  - [ ] compute TD targets inside training functions to allow jit to
	trace everything properly in a single routine

* Config collection
  + runner DONE
    run_hash (?)
    _checkpoint_dir
    iterations
    steps
    eval_period
    eval_steps
    redundancy
    policy_eval_callbacks_name
    schedule
    f"{env.environment.spec.name}-{env.environment.spec.version}"
  + agent DONE
    rng
    memory
    clip_rewards
    gamma
    min_replay_history
    sync_weights_every
    training_period
    policy_evaluator
  + rng DONE
    seed
  + memory DONE
    observation_shape
    observation_dtype
    reward_dtype
    stack_size
    batch_size
    update_horizon
    replay_capacity
  + policy_evaluator DONE
    all fields except model_call (set in base class)
  + models DONE
    model_def (got it from the start)
    optimizer (with params)
    loss_fn
* Sub-optimal design choices
  + GIN ITSELF: don't have a dictionary of parameters anymore, which
    was especially useful in Aim to visualize runs!!! so it makes the
    latter less useful...
  + config collection: happening only after instantiation, it is not
    possible to have a single config that acts as a "constructor" and
    then gets enriched with the defaults; this is different to what is
    achieved by e.g. Ray-RLib, but a similar approach relies on a
    manager that knows how to create things - right now all of my
    objects come already instantiated.
    plus, the dict of parameters is
    not available to any object at runtime - major drawback of keeping
    things separated.
  + code repetition in agents; one thing to do could be to have
    Agent.train_fn as a parameter, then to write only one Agent.train
    definition
  + Dopamine: while it provided good inspiration and insights -
    especially into JAX code and how to structure it - I mainly wanted
    to use it for:
    - checkpointing + start and stop functionality
    - metrics logging
    - serious replay buffer
    - a runner already implemented
    - observation preprocissing (although this is kinda interesting,
      and gym has something in place for that too...)
    Of these, I'm using only Dopamine's checkpointer - which is kinda
    overkill without start/stop - and the replay buffer. Especially, I
    had to design the runner myself, and twice, together with the
    metric reporters (customizable ones were added only recently). For
    the final version after the presentation, maybe I can look into
    https://github.com/rlworkgroup/garage to migrate the overall
    infrastructure - if there is time and it is not too much a hustle
    to use JAX there.
  + Runner: giving each field already instantiated has also the
    downside that the configuration contains objects that potentially
    do side effects - e.g. reporters - and is not just pure data!
    major downside; one solution is to make such attributes lazy,
    i.e. passing their args together with them - as in the first
    runner version - or partialling their args. The first option seems
    sensible as long as config collection is handled properly in a
    non-convoluted way, is in the first runner I wrote
  + reporters: maybe better to call them at each step and let them
    handle their work independently?
  + always have to give different experiment_name and logs folder when
    wanting to change between testing and not, pass SCRATCH bool param
    instead and switch on this one
  + there was some sort of bug in
    dopamine.discrete_domains.gym_lib.GymPreprocessing; consider
    migrating to gym's wrapper in the case of atari experiments too
