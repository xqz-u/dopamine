@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@InProceedings{pmlr-v97-fujimoto19a,
  title =	 {Off-Policy Deep Reinforcement Learning without Exploration},
  author =       {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle =	 {Proceedings of the 36th International Conference on Machine Learning},
  pages =	 {2052--2062},
  year =	 {2019},
  editor =	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =	 {97},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {09--15 Jun},
  publisher =    {PMLR},
  pdf =		 {http://proceedings.mlr.press/v97/fujimoto19a/fujimoto19a.pdf},
  url =		 {https://proceedings.mlr.press/v97/fujimoto19a.html},
  abstract =	 {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.}
}

@inproceedings{thrun1993issues,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
  volume={6},
  pages={1--9},
  year={1993}
}

@article{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@inproceedings{anschel2017averaged,
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={International conference on machine learning},
  pages={176--185},
  year={2017},
  organization={PMLR}
}

@inproceedings{sabatelli2020deep,
  title={The deep quality-value family of deep reinforcement learning algorithms},
  author={Sabatelli, Matthia and Louppe, Gilles and Geurts, Pierre and Wiering, Marco A},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

@article{bellman1957dynamic,
  title={Dynamic programming, princeton univ},
  author={Bellman, Richard},
  journal={Press Princeton, New Jersey},
  year={1957}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@misc{kumar,
    author = "{Kumar, Aviral}",
    title = "{Data-Driven Deep Reinforcement Learning}",
    year = "2019",
    howpublished = "\url{https://bair.berkeley.edu/blog/2019/12/05/bear/}",
    note = "[Online; accessed 11-July-2022]"
  }

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@misc{https://doi.org/10.48550/arxiv.1606.01540,
  doi = {10.48550/ARXIV.1606.01540},

  url = {https://arxiv.org/abs/1606.01540},

  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},

  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {OpenAI Gym},

  publisher = {arXiv},

  year = {2016},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{castro18dopamine,
  author    = {Pablo Samuel Castro and
	       Subhodeep Moitra and
	       Carles Gelada and
	       Saurabh Kumar and
	       Marc G. Bellemare},
  title     = {Dopamine: {A} {R}esearch {F}ramework for {D}eep {R}einforcement {L}earning},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06110},
  archivePrefix = {arXiv}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{DBLP:journals/corr/abs-1709-06560,
  author    = {Peter Henderson and
	       Riashat Islam and
	       Philip Bachman and
	       Joelle Pineau and
	       Doina Precup and
	       David Meger},
  title     = {Deep Reinforcement Learning that Matters},
  journal   = {CoRR},
  volume    = {abs/1709.06560},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.06560},
  eprinttype = {arXiv},
  eprint    = {1709.06560},
  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-06560.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.1904.06979,
  doi = {10.48550/ARXIV.1904.06979},

  url = {https://arxiv.org/abs/1904.06979},

  author = {Colas, CÃ©dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},

  keywords = {Methodology (stat.ME), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms},

  publisher = {arXiv},

  year = {2019},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
	    Haberland, Matt and Reddy, Tyler and Cournapeau, David and
	    Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
	    Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
	    Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
	    Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
	    Kern, Robert and Larson, Eric and Carey, C J and
	    Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
	    {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
	    Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
	    Harris, Charles R. and Archibald, Anne M. and
	    Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
	    {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
	    Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}