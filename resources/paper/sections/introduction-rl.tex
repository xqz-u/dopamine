% NOTE expressed the set of actions w/out a time subscript on the
% actions assuming that all actions are available at all states
% NOTE using R(s,a) as reward signal instead of R_t in order to avoid
% explaining it is a random variable, the explanation seems more
% straightforward in this way to me
% NOTE there is no explanation of the ER buffer, nor of the role and
% function of the target networks
% TODO should I write more of the presented equations in a an \equation
% environment? Check that later, keep it tightly spaced for now DONE
\subsubsection{Reinforcement Learning}\label{sec:RL_BG}
% TS: RL general elements
Reinforcement Learning seeks to solve a Markov Decision Process
(MDP) ($\mathcal{S},\mathcal{A},p,R$). In
RL, an agent interacts with an environment at
discrete time steps $t=0,1,2,3,\ldots$. At each time step $t$, the
agent receives a representation of the environment $s_t \in
\mathcal{S}$, on which it can perform some action $a_t \in
\mathcal{A}(s)$ that makes it transition to a new state
$s_{t+1}$ according to a dynamics model
$p\colon\mathcal{S}\times\mathcal{S}\times\mathcal{A}\rightarrow
\left[0,1\right],p\left(s^\prime\mid
  s,a\right)\doteq\Pr\left\{s^\prime=s_{t+1}\mid
  s=s_t,a=a_t\right\}$. At $s_{t+1}$, the agent receives
reward $r_t$ from a reward function
$R\colon\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R},
r_t=R\left(s_t,a_t\right)$. The goal of a RL agent is then to find
a mapping from states to action probabilities, called a \textit{policy}
$\pi\colon\mathcal{S}\times\mathcal{A}\rightarrow\left[0,1\right],
\pi\left(a\mid s\right)=\Pr\left\{a_t=a,s_t=s\right\}$, which
maximizes the \textit{expected return} $G_t\doteq
\sum_{t=0}^{\infty}\gamma^{t}R\left(s_t,a_t\right)$ where
$\gamma\in\left[0,1\right]$ is a discount factor that scales the
importance of future rewards. Each policy $\pi$ has a matching
\textit{state value function}
$V^{\pi}\left(s\right)=\mathbb{E}\left[G_t\mid s=s_t\right]$, which
indicates the expected return obtained starting from state $s$ therefore
following $\pi$. This function can also be expressed in terms of
state-action pairs as a \textit{state-action value function}
$Q^{\pi}\left(s,a\right)=\mathbb{E}\left[G_t|s=s_t,a=a_t\right]$,
indicating the expected return taking action $a$ in state $s$ and
consequently following $\pi$. Altogether, the RL optimization problem aims
to achieve a policy $\pi^*$ characterized by the \textit{optimal Q
  value function}
$Q^*\left(s,a\right)\doteq\max_{\pi}Q^{\pi}\left(s,a\right)$ for all
$s\in\mathcal{S}$ and $a\in\mathcal{A}\left(s\right)$, whose solution
is provided by the Bellman optimality equation
\begin{equation}
\begin{aligned}
Q^*\left(s_t,a_t\right)=\mathbb{E}\bigg[&R\left(s_t,a_t\right)+\\ &\gamma
  \max_{a\in\mathcal{A}}Q^*\left(s_{t+1},a\right)\bigg\vert
  s_t=s,a_t=a \bigg]
\end{aligned}
\end{equation}
\citep{bellman1957dynamic}. Note that the latter can
also be expressed as the \textit{optimal state value function}
$V^*\left(s\right)$ by replacing the optimal $Q$ value estimate at
the next state
$\max_{a\in\mathcal{A}}Q^*\left(s_{t+1},a\right)$ with
$V^*\left(s_{t+1}\right)$.

% TS: Deep RL and Q-learning
$Q^*\left(s,a\right)$ and $V^*\left(s\right)$ can both be learned by
Temporal Difference (TD) learning
\citep{sutton1988learning}, and Q-learning is the most popular TD
method; it learns the state-action value function using the update
rule
\begin{equation}\label{eq:ql_bellman_backup}
Q\left(s_t,a_t\right)\leftarrow
Q\left(s_t,a_t\right)+\alpha\left[y^{\scriptscriptstyle
\textrm{TD}}_{\scriptscriptstyle \textrm{QL}}-Q\left(s_t,a_t\right)\right],
\end{equation}
where
\begin{equation}\label{eq:ql_td}
y^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle
\textrm{QL}}=R\left(s_t,a_t\right)+\gamma\max_{a\in\mathcal{A}}Q\left(s_{t+1},a\right)
\end{equation}
\citep{watkins1992q}. When the state space $\mathcal{S}$ is large and
high-dimensional the $Q$ function is approximated with
deep neural networks, hence the name of Deep Reinforcement Learning
(DRL). DRL agents such as Deep Q-Learning (DQN)
\citep{mnih2013playing} have attained super-human performance on a
range of complex tasks such as the ALE benchmark suite
\citep{bellemare2013arcade}. DRL algorithms generally adapt the $Q$
function to include a neural network parameterized by $\theta$, and
reformulate the standard Q-learning update rule to a differentiable
loss function
\begin{equation}
\mathcal{L}(\theta)=\mathbb{E}_{\langle s_t,a_t,r_t,s_{t+1}\rangle\sim
\mathcal{D}}\left[{\left(y^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle
\textrm{DQN}}-Q\left(s_t, a_t;\theta\right)\right)}^2\right]
\end{equation}
where
\begin{equation}\label{eq:dqn_td}
y^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle
\textrm{DQN}}=r_t+\gamma\max_{a\in\mathcal{A}}Q\left(s_{t+1},a;\theta^{-}\right),
\end{equation}
$\mathcal{D}$ is the Experience Replay buffer \citep{lin1992self} used
to store and sample trajectories during training, and $\theta^-$ are
the parameters of a frozen target network commonly used to stabilize
value estimates. The use of this different set of parameters is
conceptually related to the bootstrapping error, as further explained
in Section~\ref{sec:BE}.

% TS: Offline RL and difference with online RL
% NOTE (maybe) place this after bootstrapping error or any other
% section that introduces off-policy RL, so that the link is very
% clear between off-policy and offline
Finally, in \textit{offline} RL -- also known as \textit{batch} RL --
the standard MDP formulation remains valid, but the agent loses the
ability to transition
from state $s_t$ to state $s_{t+1}$ by \textit{actively} choosing and
performing action $a_t$. Instead, an offline RL agent is given a
logged dataset $\mathcal{B}$ of experience tuples
$\langle s_t,a_t,s_{t+1},r_t\rangle$ generated by a \textit{behavior
policy} $\pi_{\beta}$, and its task is to learn a (possibly better)
policy than $\pi_{\beta}$ from these trajectories.
Since learning occurs under a state-action distribution induced by a
policy different from the current one, offline RL is also
known as \textit{fully off-policy} RL, and an offline agent needs to
maximize data exploitation because it lacks the possibility to
explore.
