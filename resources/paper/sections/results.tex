\section{Results}\label{sec:results}
The learning curves in Figure~\ref{fig:dshift_offline_normal} show the
$Q$-values evolution for the standard agents as a result of offline
training, while the ones in Figure~\ref{fig:dshift_offline_ensemble}
show the same metric for their respective ensemble version. Every
proposed offline agent learns to solve both classic control
environments, and the reward curves are presented in
Appendix~\ref{sec:appendix}.
% NOTE maybe this sentence is not even necessary
The reward signals for the \texttt{CartPole-v1} environment are
noisier compared to the ones for \texttt{Acrobot-v1}; this is in line
with the performance of the online behavioral DQN agent, which also
produced unstable learning curves on this problem.

\subsection{Offline bootstrapping error in the DQV family}
\begin{figure}[!tbp]
  \centering
  \includegraphics[width=.5\textwidth]{img/dshift_plots_qv.png}
  \caption{The $Q$-value estimates of offline DQN, DQV and DQV-Max at
    evaluation time. The shaded areas are $\pm 1$ standard deviation
    from the mean of 3 different
    simulations.}\label{fig:dshift_offline_normal}
\end{figure}
% Q estimates discussion for:
% DQN offline
As seen in~\ref{fig:dshift_offline_normal}, the experiments confirmed
previous findings for Q-learning-based gents run offline on continuous
domain problems \citep{pmlr-v97-fujimoto19a,kumar2019stabilizing}: the
$Q$ function incurs in a heavy overestimation bias produced by
bootstrapping errors. This is most evident on the \texttt{CartPole-v1}
environment, where DQN's $Q$-value estimates quickly escalate above
the true value. Since the offline agent has no access to ground truth
values due to lack of exploration, it cannot adjust the $Q$ function
estimates during training and the whole estimation process
diverges. Offline DQN suffers overestimation on the
\texttt{Acrobot-v1} problem too, but no divergence in $Q$ estimates is
observed here; this is likely because the behavioral data for this
environment are less noisy compared to those of \texttt{CartPole-v1}'s.

% DQV offline
Among the studied algorithms, offline DQV is the most robust one to
the bootstrapping error. On the \texttt{CartPole-v1} environment it is
almost able to correctly estimate the true value of $s_0$ for each
episode, never incurring in overoptimistic estimates. However, on the
\texttt{Acrobot-v1} problem, offline DQV still suffers from stable
overestimation, despite coming closest to the true value of $s_0$.
DQV avoids the bootstrapping error because it is an \textit{on-policy}
algorithm. Although theoretically it should not be able to learn in
the offline setting, its strong
performance compared to the other agents is probably due to efficient
usage of the large offline dataset, which enables it to learn on-policy
discovering effective behaviors in the data. Moreover, DQV forms its
TD-target using only the state-value function $V$, therefore it cannot
possibly base predictions on those very out-of-distribution actions
which are responsible for bootstrapping errors.

% DQV-Max offline
Offline DQV-Max is also more resilient to the bootstrapping error than
offline DQN.\ On the \texttt{CartPole-v1} environment, it estimates
the true value for $s_0$ nearly perfectly, showing no detrimental
effects due to misaligned bootstrap estimates. As it is the case for
offline DQV and DQN, it still overestimates the real $Q$-value for
$s_0$ on the \texttt{Acrobot-v1} problem, positioning in between the
estimates of DQN and DQV.\ The low $Q$-values on the
\texttt{CartPole-v1} environment are most likely in virtue of
DQV-Max's decoupling of \textit{selection} and \textit{evaluation}
\citep{van2016deep}. DQV-Max forms its temporal difference regression
targets (selection) from a model different than the one it uses to
compute value estimates (evaluation). This separation is especially
important for DQV-Max's TD-targets for the $V$ function of
Equation~\ref{dqvmax:v_td_target}, where evaluating
out-of-distribution actions could disrupt the function's
convergence to the true $V^*$. \citet{sabatelli2020deep} note that
this disentanglement makes DQV-Max less prone to the overestimation
bias in the online setting, and these experiments confirm the results
for the offline one.

\subsection{Offline bootstrapping error on the ensemble variants}
\begin{figure}[!tbp]
  \centering
  \includegraphics[width=.5\textwidth]{img/dshift_plots_ensembles_qv.png}
  \caption{Evaluation time $Q$-value estimates of the ensemble version
    of offline DQN, DQV and DQV-Max. The shaded areas are $\pm 1$
    standard deviation from the mean of 3 different
    simulations.}\label{fig:dshift_offline_ensemble}
\end{figure}
% TS: ensemble-dqn less optimistically biased, in line with baseline
% results from REM and with theoretical analysis of Averaged-DQN
On the \texttt{CartPole-v1} environment, Offline Ensemble-DQN suffers
from a milder overestimation bias than offline DQN.\ The $Q$-value
estimates on this problem decreased significantly with the
implementation of the ensemble strategy ($t=2.62,p<.01$); moreover, in
line with the baseline results of \citet{agarwal2020optimistic} and
with the theoretical analysis of \citet{anschel2017averaged}, the
observed decrease in $Q$ estimates variance is proportional to the
ensemble number of heads $K$ (8755.07 vs. 2330.90 for Ensemble-DQN and
DQN, respectively). However, ensembling the $Q$ function alone
does not prevent the bootstrapping error: as seen in
Figure~\ref{fig:dshift_offline_ensemble}, the $Q$-value for $s_0$
still diverges from the true baseline. Similar results to base offline
DQN were obtained on the \texttt{Acrobot-v1} environment.

% stuff to say generally:
% - acrobot is more stable env for mean q values + for dqns and dqvs,
%   variance too
%   - generally: all algos seem to perform basically the same!

% worse on ensemble version,
%   and except for dqns, their estimates of the response metric have
%   higher variability -> in fact also rewards are lower NOTE why??
% - ablated versions: no significant differences at all NOTE why??


% TS: ensemble-dqv no real change
Concerning offline Ensemble-DQV, no significant change to its standard
counterpart was observed as it can be seen from the nearly identical
$Q$-value curves of Figure~\ref{fig:dshift_offline_ensemble}. However,
the reward signals for this agent follow a pattern generally recorded
across all ensemble variants: average returns are more unstable across

% TODO paper plots:
% - DQVMax and ablations offline, only Q-values
% appendix:
% - rewards for ablations
