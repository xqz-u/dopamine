\section{Methods}\label{sec:methods}
% TS: intro/summary of what happens in this section
% The main focus of this research are bootstrapping error and its
% prevention in the DQV family of deep
% reinforcement learning algorithms \citep{sabatelli2020deep}.
The preliminary experiments in this research aimed at determining the
presence of bootstrapping errors in DQV and DQV-Max, the two
algorithms in the DQV algorithmic family \citep{sabatelli2020deep}.
Subsequently, in line with the research question, a second experiment
sought to assess the impact of value function ensembling on the
susceptibility of these algorithms to the bootstrapping error.

% NOTE should I keep all the equation numbering?
% TODO the objective function for Q in DQV-Max does not fit inside the
% column, needs to shrink more somehow?! DONE
% TS: DQV family: why chosen and what is it
\subsection{DQV and DQV-Max}
DQV and DQV-Max were chosen because they are model-free, value-based
deep RL algorithms like DQN, suitable for learning on discrete
domains. Moreover, differently from DQN, both algorithms add
a state-value function $V$ to the optimization problem to
obtain more robust value estimates and be less prone to the
overestimation bias \citep{sabatelli2020deep}. Given two neural
networks $Q\left(s_t,a_t;\theta\right)$ and $V\left(s_t;\phi\right)$
with corresponding target parameters $\theta^-$ and $\phi^-$ that
stabilize the computation of TD-targets, and trajectory batches
sampled from the Experience Replay Buffer $\mathcal{D}$, the objective
functions used by DQV become
\begin{align}
\mathcal{L}\left(\phi\right)&=\mathbb{E}_{\langle
                              s_t,a_t,r_t,s_{t+1}\rangle\sim
                              \mathcal{D}}\left[{\left(y^{\scriptscriptstyle
                              \textrm{TD}}_{\scriptscriptstyle
                              \textrm{DQV}}-V\left(s_t;\phi\right)\right)}^2\right]\label{eq:dqv_loss_0}
  \\
\mathcal{L}\left(\theta\right)&=\mathbb{E}_{\langle
                                s_t,a_t,r_t,s_{t+1}\rangle\sim
                                \mathcal{D}}\left[{\left(y^{\scriptscriptstyle
                                \textrm{TD}}_{\scriptscriptstyle
                                \textrm{DQV}}-Q\left(s_t,a_t;\theta\right)\right)}^2\right],\label{eq:dqv_loss_1}
\end{align}
where
\begin{equation}
  y^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle
    \textrm{DQV}}=r_t+\gamma V\left(s_{t+1};\phi^{-}\right).
\end{equation}
Due to the lack of a maximization operation in the TD-target
$y^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle
  \textrm{DQV}}$, DQV is an \textit{on-policy} algorithm; moreover, it
should
be noted that both the $Q$ and $V$ function of DQV learn from the same
TD-target computed by $V$. Conversely, the objective functions for
DQV-Max are as follows:
\begin{align}
\mathcal{L}\left(\phi\right)&=\mathbb{E}_{\langle
                              s_t,a_t,r_t,s_{t+1}\rangle\sim
                              \mathcal{D}}\left[{\left(v^{\scriptscriptstyle
                              \textrm{TD}}_{\scriptscriptstyle
                              \textrm{DQV-Max}}-V\left(s_t;\phi\right)\right)}^2\right]
  \\
\mathcal{L}\left(\theta\right)&=\mathbb{E}_{\langle
                                s_t,a_t,r_t,s_{t+1}\rangle\sim
                                \mathcal{D}}\left[{\left(q^{\scriptscriptstyle
                                \textrm{TD}}_{\scriptscriptstyle
                                \textrm{DQV-Max}}-Q\left(s_t,a_t;\theta\right)\right)}^2\right]
\end{align}
where
\begin{align}
  v^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle
  \textrm{DQV-Max}}&=r_t+\gamma\max_{a\in\mathcal{A}}Q\left(s_{t+1},a;\theta^{-}\right)
  \label{dqvmax:v_td_target}\\
  q^{\scriptscriptstyle \textrm{TD}}_{\scriptscriptstyle \textrm{DQV-Max}}&=r_t+\gamma
  V\left(s_{t+1};\phi\right)\label{dqvmax:q_td_target}.
\end{align} As seen in TD-target
Equation~\ref{dqvmax:v_td_target} -- equal to the DQN TD-target
Equation~\ref{eq:dqn_td} -- DQV-Max is an off-policy algorithm, so
of greater interest for offline RL.\ In addition,
DQV-Max uses two different temporal difference targets, where the one
used to learn the state-action value function resembles the TD-target
for DQV but without the usage of a target network.

\subsection{Common experimental details}
% TS: experiments implementation details:
% environments/agents/hyper-parameters/DQN online data collection
All experiments are conducted on the
\texttt{CartPole-v1} and \texttt{Acrobot-v1} OpenAI Gym environments
\citep{https://doi.org/10.48550/arxiv.1606.01540}, two classic control
problem well studied in the RL literature. Both environments
provide continuous state representations $s\in \mathbb{R}^n$ and
discrete action spaces, suitable for approximate dynamic programming
(i.e.\ value-based) methods such as Q-learning. The
deep reinforcement learning algorithms involved in the experiments are
self-implemented using the JAX machine learning library by Google
\citep{jax2018github}, then trained and evaluated under standard deep
RL neural networks architectures, hyper-parameters and pre-processing
settings following the Dopamine reinforcement
learning framework \citep{castro18dopamine}; see
Appendix~\ref{sec:appendix_hparams} for the full hyper-parameters
table.

Due to the offline nature of the proposed experiments,
the first common step was online data collection. We replicated the
data collection process employed by \citet{agarwal2020optimistic} for
the \href{https://research.google/tools/datasets/dqn-replay/}{DQN
  Replay Dataset} on the ALE environments, adapting it to the two
proposed problems. We
trained a behavioral DQN agent online on each environment for a total
of approximately $500 \; 000$ steps (500 iterations of at least 1000
steps),
starting to fit the $Q$ function after experiencing 500
trajectories, then performing a gradient update every 4 steps. We
logged every trajectory observed by the behavioral agent during
training in order to gather a dataset of significant size and diverse
policy composition, vital for the success of offline RL
\citep{agarwal2020optimistic}. This process was repeated across 3
random seed initializations to control for volatility due to
stochasticity. In the offline experiments, each run was
paired with one of these logged datasets, such that the reported
response metrics are averages over 3 redundancies.

% TS: first experiment description -> monitor the evolution of Q
% values at the first state of each episode to see the overestimation
% (hopefully) not taking place
% NOTE ask Matthia if he wanted to use evaluation time to record
% estimates because the agents act greedily most of the time then. Since
% they still have a very small chance of exploring then, I still record
% every Q(s,a) during action selection
% NOTE the baseline dashed line for the return plots is not explained
% right now; the reward plots can go in the appendix, then should I
% explain what I mean by 'solving' the environment?
\subsection{Bootstrapping error in offline DQV and
  DQV-Max}\label{sec:methods_be_dqv_dqvmax}
In order to detect the bootstrapping error, we need to track the
evolution of the $Q$ estimates as a function of training steps -- a
proxy for the number of Bellman backups. If the
bootstrapping error occurs, it will cause an overestimation bias in
the $Q$ function, and the agents will inflate the expected discounted
return $G_t$ it thinks to gain starting from state $s_t$. Since the
chosen environments provide constant reward $r$ at each time-step $t$
until the enforced episode termination at time $M$, it is
straightforward to compute the baseline discounted return expected
from state $t$ as $G_t=\sum^{M}_{k=t}\gamma^{k}r$.
We recorded the progression of $Q$ values at evaluation time focusing
on the first state $s_0$ of each new episode, as given by $\max_{a\in
  \mathcal{A}\left(s\right)}Q\left(s_0,a;\theta\right)$. It should be
noted that one evaluation iteration of 1000 time-steps occurred every
5 training iterations; by interleaving training and testing, we are
still able to analyze the $Q$ estimates evolution as a temporal
sequence, and to assess the effects of bootstrapping error
accumulation. Results are presented in
Figure~\ref{fig:dshift_offline_normal}.
% , together with the actual
% rewards obtained at test time.
Offline DQN is used as a baseline; the full lines correspond to each
agent's $Q$ estimates at evaluation time, while the dashed line is the
environment's actual return $G_t$ at an episode's first state $s_0$.

% TS: check if BE appears when using simple ensemble versions of the
% proposed algorithms
% NOTE talking about weight vector: should it be bold as found for
% vectors elsewhere? Then should I change all such vectors in the
% paper?
% TODO ask Matthia how should I go about breaking too long equations
% DONE
\subsection{Ensemble DQV and DQV-Max}
% TS: brief description of main inspiration, Ensemble-DQN
To investigate the efficacy of ensembling methods for
bootstrapping error prevention, an averaging ensemble
technique inspired by Ensemble-DQN \citep{agarwal2020optimistic} was
implemented.
In Ensemble-DQN, the $Q$ function is approximated by
an ensemble of $K$ heads parameterized by a weight vector $\theta$,
with corresponding target weights $\theta^-$; each $Q$-value
prediction
% $Q\left(s_t,a_t;\theta_{k}\right)$
is then optimized with respect to its own target,
% $Q\left(s_t,a_t;\theta_{k}^-\right)$.
similarly to Bootstrapped-DQN \citep{osband2016deep}. Each head is
initialized with different parameters and trained on all data using
identical mini-batches; although bootstrap ensembles actually require
that each head is trained on a different sample drawn with
replacement from the dataset $\mathcal{D}$, it is well known in the
deep learning literature that
initializing a model with different parameters provides enough
diversity to the model to obtain reliable uncertainty estimates
\citep{osband2016deep,levine2020offline}. Finally, each head optimizes
a global loss which is the average of the ensemble total loss; the
objective function for Ensemble-DQN thus takes the form
\begin{equation}
  \begin{aligned}
\mathcal{L}\left(\theta\right)=&\frac{1}{K}\sum_{k=0}^{k-1}\mathbb{E}_{\langle
                                 s_t,a_t,r_t,s_{t+1}\rangle\sim\mathcal{D}}\\
                               &\left[{\left(y^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQN}}-Q\left(s_t,a_t;\theta_{k}\right)\right)}^2\right],
  \end{aligned}
\end{equation}
where
\begin{equation}
  y^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQN}}=r_t+\gamma\max_{a\in\mathcal{A}}Q\left(s_{t+1},a;\theta_{k}^{-}\right).
\end{equation}

When translating the ensemble loss objective to DQV and DQV-Max, we
decided to use ensembles for the TD-targets computation. The rationale
for this choice is to mitigate the bootstrapping error occurring
precisely at this step, as we expect that the compound estimate of the
ensemble is close to the true TD-target $\hat{y}$. As a result,
Ensemble-DQV employs only one ensemble on the $V$ function to compute
a TD-target common across $Q$ and $V$; on the contrary, since DQV-Max
requires two different TD-targets, Ensemble-DQV-Max uses ensembles to
estimate both the $Q$ and $V$ function, yet the $V$ heads lack target
networks as in the original DQV-Max algorithm. Thus, the modified
objectives of Ensemble-DQV and Ensemble-DQV-Max respectively become
\begin{equation}
  \begin{aligned}
    \mathcal{L}\left(\phi\right)=&\frac{1}{K}\sum_{k=0}^{k-1}\mathbb{E}_{\langle
                                   s_t,a_t,r_t,s_{t+1}\rangle\sim\mathcal{D}}\\
                                 &\left[{\left(y^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV}}-V\left(s_t;\phi_{k}\right)\right)}^2\right]
  \end{aligned}
\end{equation}
\begin{equation}
  \begin{aligned}
    \mathcal{L}\left(\theta\right)=&\frac{1}{K}\sum_{k=0}^{k-1}\mathbb{E}_{\langle
                                     s_t,a_t,r_t,s_{t+1}\rangle\sim\mathcal{D}}\\
                                   &\left[{\left(y^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV}}-Q\left(s_t,a_t;\theta\right)\right)}^2\right],
  \end{aligned}
\end{equation}
where
\begin{equation}
  y^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV}}=r_t+\gamma
  V\left(s_{t+1};\phi_{k}^{-}\right)
\end{equation}
and
\begin{equation}
  \begin{aligned}
    \mathcal{L}\left(\phi\right)=&\frac{1}{K}\sum_{k=0}^{k-1}\mathbb{E}_{\langle
                                   s_t,a_t,r_t,s_{t+1}\rangle\sim\mathcal{D}}\\
                                 &\left[{\left(v^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV-Max}}-V\left(s_t;\phi_k\right)\right)}^2\right]
  \end{aligned}
\end{equation}
\begin{equation}
  \begin{aligned}
    \mathcal{L}\left(\theta\right)=&\frac{1}{K}\sum_{k=0}^{k-1}\mathbb{E}_{\langle
                                     s_t,a_t,r_t,s_{t+1}\rangle\sim\mathcal{D}}\\
                                   &\left[{\left(q^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV-Max}}-Q\left(s_t,a_t;\theta_k\right)\right)}^2\right],
  \end{aligned}
\end{equation}
where
\begin{align}
  v^{\scriptscriptstyle
  \textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV-Max}}&=r_t+\gamma\max_{a\in\mathcal{A}}Q\left(s_{t+1},a;\theta_k^{-}\right)\\
  q^{\scriptscriptstyle\textrm{TD}}_{\scriptscriptstyle\textrm{Ens-DQV-Max}}&=r_t+\gamma V\left(s_{t+1};\phi_k\right).
\end{align}
The ensembles were implemented using a multi-head architecture: each
head shares the same body of layers except for a final
fully connected layer, initialized with different parameters across
heads; this architecture is also employed by REM
\citep{agarwal2020optimistic}.
The experiments concerning the ensemble version of DQV and DQV-Max
follow the same setup as in Section~\ref{sec:methods_be_dqv_dqvmax},
however in this case we recorded
$\max_{a\in\mathcal{A}}\frac{1}{K}\sum_{k=0}^{k-1}Q\left(s_0,a;\theta_k\right)$
to track the evolution of value estimates.
For each experiment on the aforementioned environments a number of
heads $K=4$ was used; although other examples in the literature use a
greater number of heads (e.g. $K=10$ for Bootstrapped-DQN
\citep{osband2016deep}), we settled on 4 due to computational
limitations. Results are presented in
Figure~\ref{fig:dshift_offline_ensemble}, where offline Ensemble-DQN
is used as the baseline.
% TODO ablations study once the data are ready; think about how to
% discuss them in the results section DONE
