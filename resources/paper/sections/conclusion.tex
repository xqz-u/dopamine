\section{Conclusion}\label{sec:Conclusion}
% TS: general conclusion: no gains from bare ensemble technique
% implemented as is
The experiments presented in the previous section provide an answer to the
question of whether the DQV algorithmic family benefits from ensemble
techniques in offline RL.\ According to the
empirical results, simply re-framing the learning problem of
offline RL to use multiple copies of the same agent is not enough to
prevent the bootstrapping error. Ensembles of function approximators
are a well-known performance boost in machine learning, but it is
crucial to properly exploit the additional information they provide
compared to a single learner. If this point is not correctly
addressed, the possible uncertainty estimation gains are overlooked,
and such information is critical for an offline RL agent.

% LIMITATIONS
% TS: one limitation is the DQV family, which is too strong and
% already acts where the bootstrapping error takes place, so they are
% inherently wrong to use with naive ensembles
When testing whether the simple ensemble technique of Ensemble-DQN can
be generalized to other algorithms, translating it to DQV and DQV-Max
becomes a limitation of this study. In fact, as already mentioned,
these algorithms disentangle \textit{selection} -- the choice of the
regression targets for the state or state-action value function --
from \textit{evaluation} -- the estimation of a state or
state-action pair's value. This is one important factor in the infamous
\textit{deadly triad} of off-policy RL
\citep{sutton2018reinforcement}, and the main focus of this research
under the form of bootstrapping error. Where such decoupling is absent,
as in the DQN algorithm, using a simple ensemble was in fact enough to
observe gains in terms of variance reduction, which is directly
related to the overestimation bias \citep{anschel2017averaged} and is
caused by misaligned value estimates -- also known as bootstrapping
errors. This was the case for the experiments with offline
Ensemble-DQN on the \texttt{CartPole-v1} environment. When it comes to
the DQV algorithmic family and controlling the bootstrapping error
with ensembles, these algorithms' strength thus becomes an
architectural weakness for the proposed experiments. In fact, base DQV
and DQVMax show a robust performance in the offline setting that is
unaffected by the addition of more heads to estimate uncertainty.
By using a function to form the TD-targets different from the one
dedicated to evaluate current states, these algorithms already
take significant steps to prevent the bootstrapping error, which
makes them unfit candidates for the simple ensemble strategy of
Ensemble-DQN.\

% TS: another limitation is the naive ensemble technique used, which is
% a simple average over heads for both Q(s',a') and Q(s,a) and trains
% everyone on the same loss
Another limitation is the naive ensemble technique employed throughout
the experiments adapted from Ensemble-DQN.\ Given the empirical
results, it is not sufficient to increase the number of prediction
heads in DQV and DQV-Max, to apply standard value-based regression
methods (i.e.\ temporal difference learning) individually on each,
and finally to train every head on the average of the ensemble total
loss. As previously stated, the former are two strong off-policy
algorithms; yet for
offline RL they could still benefit from the uncertainty estimated by
an ensemble of $Q$ or $V$ functions, if this information is properly
integrated in their learning formulation. The simple averaging technique
implemented in this research does not fully exploit the uncertainty
information provided by the ensembles of $Q$ and $V$ functions. It
would be interesting to see what happens when conservative estimates
are formed in the face of uncertainty, for example, by down-scaling
the TD-targets by the variance of the ensemble predicted $Q$-values as
discussed in \citet{levine2020offline}.

% FUTURE RESEARCH
% TS: cool to see if dqv and dqvmax are strong alone with less and/or
% worse online training data
For future work with a setup similar to this research, different lines
of experimentation are possible. For example, the size and diversity
of the offline dataset $\mathcal{B}$ collected by $\pi_{\beta}$ could
be manipulated to asses the generalization capabilities of offline DQV
and DQV-Max. As seen in some of the experiments for the REM agent
\citep{agarwal2020optimistic}, the size of the dataset $\mathcal{B}$
could be reduced to find the minimum amount of trajectories needed to
obtain an acceptable performance level. Alternatively, offline DQV and
DQV-Max could be
given only expert or quasi-random data, thus decreasing $\mathcal{B}$'s
diversity. The offline agents in this research learned on the full set
of policies encountered during online training of $\pi_{\textrm{DQN}}$,
in lieu of the importance of training datasets' diverse composition
highlighted by the REM results.
Learning on data produced by a small number of policies or by
highly suboptimal ones matters for offline DQV and DQV-Max because it
resembles what is available from many settings in the real world,
where behavioral data are collected by a handful of static policies --
or even a single one.

% TS: move uncertainty estimation out of the single algorithm scope,
% and use it when combining different conceptually different learners;
% ultimately, try voting ensembles, e.g. DQV and DQV-Max
Finally, regarding uncertainty estimation in offline RL, the ensemble
component could be extracted from the single algorithm scope and
applied to different learners. This means having a multitude of agents
(e.g.\ both DQV and DQV-Max) learn and collaborate on the same
problem, implementing a voting procedure to decide, for example, on
the TD-targets for the whole ensemble. The same information
relating to uncertainty estimation purposes available from individual
heads in the current setup could then come from different RL
algorithms that inform the ensemble decisions. Voting ensembles are a
well-defined concepts in machine learning, and in the case of DQV and
DQV-Max it could be of interest to assess if the relative strength of
each agent taken individually -- lower variance for the first
on-policy algorithm, greater learning generality for the second
off-policy one -- can be combined in a meaningful way in the offline
RL setting.
