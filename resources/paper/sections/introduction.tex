% FIXME there is a break-line somewhere after the second paragraph that
% appears idk why
% TODO read all again once the other subsections are defined, and
% evaluate what is written here. Should it be split up? Is the story
% coherent, are there no logical gaps and - especially, this is a
% mistake you tend to make - are you giving too much for granted without
% explaining it?
\section{Introduction}\label{sec:introduction}
% TS: advancements in ML are data-driven, we need the same for RL - can
% achieve it with offline RL
% NOTE could add some more references?
% NOTE shorten or split into sub-paragraphs, ca. 100w each
In the past decade, machine learning methods have encountered
major success over a wide number of real-world applications, ranging
from Computer Vision to Natural Language Processing tasks.
Much of the progress in these areas can be attributed to the
development of \textit{data-driven}, scalable learning methods. In
fact, although advancements in models and architectures are an integral
part of this success story, the learning techniques they employ are
well-founded and understood, and major improvements in model
performance stem from the availability of large and diverse training
datasets.
However, such data-driven methods do not map to the Reinforcement
Learning (RL) framework equally well. RL involves sequential decision
making problems where the best behavior strategy is learned through
active interaction with the environment. This naturally
\textit{online} learning paradigm prevents effective exploitation of
the rich \textit{offline} datasets, which is at the base of the
success of supervised learning methods.
Moreover, data collection for complex real-world RL applications such as
autonomous driving or healthcare support systems can often be
expensive or hazardous. Developing safe and capable
offline RL agents has therefore a great appeal: by efficiently
learning from large amounts of data, we could create ``generalizable
and powerful \textit{decision making engines}''
\citep{levine2020offline} that can aid in solving many real-world open
problems \citep{levine2020offline}.

% TS: offline RL entails reasoning about what has not happened
% (counterfactual queries); this is problematic for standard ML and
% creates distribution shift DS
In offline RL, an agent learns a policy from a static dataset of
logged experiences produced by a \textit{behavior policy}.
This differs from classical online RL, where an agent can actively
collect new experience by interacting with its environment
\citep{sutton2018reinforcement}. For an offline agent to
improve, it is therefore crucial that it is able to partake in
counterfactual
reasoning to accurately estimate what would happen if it took a
decision different from the one in the training dataset
\citep{levine2020offline}. By contrast, an online RL agent could
explore and learn on its own the effects of a decision
different than the one previously chosen in the same situation. This
additional constraint of offline RL is
problematic, and it exceeds the capabilities of current machine learning
methods that use expressive function approximators (neural networks)
to generalize across examples. For one, it violates the assumption of
independent and
identically distributed data (i.i.d.) that standard supervised
learning algorithms rely on: an offline RL agent may be trained under
one distribution but tested under a different one.
In addition, an offline agent must be able to reason differently from
the data-generating policy in order to produce novel -- and possibly
favorable -- courses of actions; this requirement breaks the i.i.d.\
assumption too.
The mismatch between the behavior policy-induced distribution and the
one learned during training is called \textit{distributional shift},
and it presents a fundamental challenge for the efficacy of offline RL
\citep{levine2020offline}.

% TS: DS affects online and offline off-policy algos, especially in
% the form of bootstrapping error
Distributional shift generally affects off-policy RL algorithms. These
are methods that learn about a \textit{target policy} using a
different \textit{behavior policy} \citep{sutton2018reinforcement},
such as the popular Q-learning agent \citep{watkins1992q}. Due to
their ability to learn from data generated by another policy,
off-policy algorithms naturally lend themselves to offline RL.\@ It is
well known that off-policy methods exhibit high variance
\citep{sutton2018reinforcement}; moreover,
off-policy algorithms which employ a maximization operation in the
bootstrapping step, such as Q-learning,
are prone to overoptimistic value estimates
\citep{thrun1993issues}. In the offline setting, this form of
\textit{bootstrapping error} results in the selection of actions that
lie outside of the training data distribution
\citep{kumar2019stabilizing}, and since no ground truth information to
estimate their value is available - the dataset is fixed - they
disrupt the training process and drive it towards regions of
uncertainty; a more in-depth account of the bootstrapping error
follows in Section~\ref{sec:BE}.

% NOTE look at commented part at end of paragraph for another
% formulation of RQ
% TS: ensemble methods address the bootstrapping error, off-policy and
% offline generally; do the results related to standard Deep
% Q-Learning with ensembles transfer to other value-based methods,
% such as DQV family?
To address distributional shift and bootstrapping errors in
off-policy learning, many techniques found in the literature employ
ensemble-based methods (\citeauthor{osband2016deep},
\citeyear{osband2016deep}; \citeauthor{anschel2017averaged},
\citeyear{anschel2017averaged}; \citeauthor{pmlr-v97-fujimoto19a},
\citeyear{pmlr-v97-fujimoto19a}, appendix D.2).
% \citep{osband2016deep, anschel2017averaged,
%   pmlr-v97-fujimoto19a}.
Most relevant for this research,
ensembling methods have demonstrated a successful approach in
offline RL, as seen in the REM agent by
\citet{agarwal2020optimistic}. This
paper investigates to which extent results concerning bootstrapping
error prevention based on ensembled versions of Q-learning transfer to
other model-free, value-based RL algorithms in the context of offline
RL.\@ In particular, we will inquire whether benefits concerning
bootstrapping error reduction stemming from simple ensembling methods
apply to the deep RL agents of the DQV algorithmic family
\citep{sabatelli2020deep}.
% also apply to RL algorithms that already take steps to minimize it,
% such as the DQV algorithmic family \citep{sabatelli2020deep}.

\subsection{Background}
The following subsections will introduce some definitions and prior
knowledge needed to understand this paper.

\input{sections/introduction-rl.tex}

\input{sections/introduction-bootstrapping_error.tex}

\input{sections/introduction-related_work.tex}