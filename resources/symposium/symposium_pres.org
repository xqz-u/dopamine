#+TITLE:     Using Ensembles to address Bootstrapping Error in Offline Reinforcement Learning
#+AUTHOR:    Marco A. Gallo
#+EMAIL:     m.a.gallo@student.rug.nl
#+DATE:      29-06-2022
#+OPTIONS:   H:2 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+startup: beamer
#+LaTeX_CLASS: beamer
# #+LaTeX_CLASS_OPTIONS: [bigger]
#+laTeX_header: \mode<beamer>{\usetheme{Madrid}}
#+beamer_frame_level: 2

#+laTeX_header: \usepackage[super]{natbib}

# TODO dots menu that shows slides progression

* Background

** Reinforcement Learning (RL)
+ An agent seeking an optimal policy $\pi(s, a)$ - a mapping from
  states to action probabilities ($s \in S$, $a \in A$)
+ Used in sequential decision making problems modeled as Markov
  decision process (/MDP/), enriched with a reward function $R(s, a)$
# NOTE these do not require knowing fully the transition probabilities
# in the MDP, which are computed in expectation by estimating the value
# of a state as a proxy
+ Focus: /value-based/, /model-free/ methods

*** RL Definitions
\fontsize{9pt}{10pt}\selectfont
\begin{align*}
R_t &= \sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1} & \tag{Expected discounted reward} \\
Q^{\pi}(s, a) &= \mathbb{E}\left[R_{t} \vert s_{t}=s, a_{t}=a\right] & \tag{State-action value function} \\
Q^* &= \mathcal{T}^* Q^* & \\
(\mathcal{T}^* \hat{Q})(s, a) &= R(s, a) + \gamma \mathbb{E}_{T(s'|s,a)}[\max_{a'}\hat{Q}(s', a')] \tag{Bellaman optimality equations}
\end{align*}

** Reinforcement Learning (RL) - Online
#+ATTR_LaTeX: :width \textwidth
[[./online_rl_loop.jpg]]

** Reinforcement Learning (RL) - Offline
#+ATTR_LaTeX: :width \textwidth
[[./offline_rl_sketch_2.png]]

+ Also called Batch Reinforcement Learning
+ Behavior policy $\beta(s|a)$ generates data
+ /Pure Batch/ vs /Growing Batch/ RL methods

* Offline RL is hard

** Detrimental factors in Offline RL
# *** Common
# NOTE these hinder generalization
+ Function approximation errors in Deep RL (Neural Networks)
# NOTE this cannot be corrected by further interaction with the environment in the pure offline setting
+ Different state visitation frequencies under training and testing
  distributions
# *** Specific
+ *Bootstrapping error* (\citeauthor{kumar2019stabilizing},
  \citeyear{kumar2019stabilizing})

** Bootstrapping Error
+ Both the targets $(\mathcal{T}^* \hat{Q})(s, a)$ and the estimates
  $\hat{Q}(s, a)$ for the Q-function regression come from the current
  estimate $\hat{Q}$
+ Q-estimates for out-of-distribution (OOD) actions - those outside of
  \beta - arbitrarily wrong
+ Naively selecting $\max_{a'}\hat{Q}(s', a')$ as part of the
  regression target propagates these overestimates
+ Happens off-policy generally, but offline the lack of further
  exploration can't correct errors

# TODO caption?
# #+ATTR_LaTeX: :width \textwidth
[[./bootstrap_error_offline_rl.png]]

** Bootstrapping Error in the DQV\cite{sabatelli2020deep} algorithmic family
#+ATTR_LaTeX: :width \textwidth
[[./dshift_plots_normal.png]]

* Possible solution: Ensembles
* Experiments
* Conclusions and future work
* References
** References
\bibliographystyle{apalike}
\bibliography{bibliography}
